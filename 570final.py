# -*- coding: utf-8 -*-
"""570final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d19zlzTJyKGWnkPZAqomPboOWv3Hw8rV
"""

# from google.colab import files

# uploaded = files.upload()

# for fn in uploaded.keys():
#   print('User uploaded file "{name}" with length {length} bytes'.format(
#       name=fn, length=len(uploaded[fn])))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from xgboost.sklearn import XGBRegressor

data= pd.read_csv('SeoulBikeData.csv', encoding = 'latin-1')
data.head()

data.describe()

"""data preprocessing"""

new_sec = pd.get_dummies(data, columns=['Seasons'],prefix=['season'],drop_first=True)
data = new_sec
data.head()

new_hol = pd.get_dummies(data, columns=['Holiday'],prefix=['holiday'],drop_first=True)
data = new_hol
data.head()

new_fuc = pd.get_dummies(data, columns=['Functioning Day'],prefix=['functioning day'],drop_first=True)
data = new_fuc
data.head()

data.columns

corr = data[['Rented Bike Count', 'Hour', 'Temperature(Â°C)',
       'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)',
       'Dew point temperature(Â°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)',
       'Snowfall (cm)', 'season_Spring', 'season_Summer', 'season_Winter',
       'holiday_No Holiday', 'functioning day_Yes']].corr()
corr

sns.heatmap(corr, cmap="YlGnBu")

"""delate Dew point tem, season_summer, season_winter, Humidity"""

data_lr = data.drop(['Dew point temperature(Â°C)','season_Summer', 'season_Winter','Humidity(%)'], axis = 1)
corr = data_lr[['Rented Bike Count', 'Hour', 'Temperature(Â°C)',
       'Wind speed (m/s)', 'Visibility (10m)', 
       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)',
       'season_Spring','holiday_No Holiday', 'functioning day_Yes']].corr()
corr

sns.heatmap(corr, cmap="YlGnBu")

new_hour = pd.get_dummies(data, columns=['Hour'],prefix=['Hour'],drop_first=True)
data = new_hour
data.head()

new_tem = pd.cut(data['Temperature(Â°C)'],4,labels=['cold', 'cool', 'warm', 'hot'])
data['Temperature(Â°C)'] = new_tem
new_temp = pd.get_dummies(data, columns=['Temperature(Â°C)'],prefix=['Tem'],drop_first=True)
data = new_temp
data.head()

data.columns

len(data.columns)

x = data[['Humidity(%)', 'Wind speed (m/s)',
       'Visibility (10m)', 'Dew point temperature(Â°C)',
       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)',
       'season_Spring', 'season_Summer', 'season_Winter', 'holiday_No Holiday',
       'functioning day_Yes', 'Hour_1', 'Hour_2', 'Hour_3', 'Hour_4', 'Hour_5',
       'Hour_6', 'Hour_7', 'Hour_8', 'Hour_9', 'Hour_10', 'Hour_11', 'Hour_12',
       'Hour_13', 'Hour_14', 'Hour_15', 'Hour_16', 'Hour_17', 'Hour_18',
       'Hour_19', 'Hour_20', 'Hour_21', 'Hour_22', 'Hour_23', 'Tem_cool',
       'Tem_warm', 'Tem_hot']]
y = data['Rented Bike Count']

"""linear regression"""

new_hour = pd.get_dummies(data_lr, columns=['Hour'],prefix=['Hour'],drop_first=True)
data_lr = new_hour
new_tem = pd.cut(data_lr['Temperature(Â°C)'],4,labels=['cold', 'cool', 'warm', 'hot'])
data_lr['Temperature(Â°C)'] = new_tem
new_temp = pd.get_dummies(data_lr, columns=['Temperature(Â°C)'],prefix=['Tem'],drop_first=True)
data_lr = new_temp

data_lr.columns

x_lr = data_lr[['Wind speed (m/s)', 'Visibility (10m)',
       'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)',
       'season_Spring', 'holiday_No Holiday', 'functioning day_Yes', 'Hour_1',
       'Hour_2', 'Hour_3', 'Hour_4', 'Hour_5', 'Hour_6', 'Hour_7', 'Hour_8',
       'Hour_9', 'Hour_10', 'Hour_11', 'Hour_12', 'Hour_13', 'Hour_14',
       'Hour_15', 'Hour_16', 'Hour_17', 'Hour_18', 'Hour_19', 'Hour_20',
       'Hour_21', 'Hour_22', 'Hour_23', 'Tem_cool', 'Tem_warm', 'Tem_hot']]

#define cross-validation method to use
cv = KFold(n_splits=10, random_state=1, shuffle=True)
#build multiple linear regression model
lr = LinearRegression()
#use k-fold CV to evaluate model
scores = cross_val_score(lr, x_lr, y, scoring='neg_mean_squared_error',
                         cv=cv, n_jobs=-1)
lr_mse = np.mean(- scores)
print('the mse of linear regression is: ', lr_mse)

#define cross-validation method to use
cv = KFold(n_splits=10, random_state=1, shuffle=True)
#build multiple linear regression model
lr = LinearRegression()
#use k-fold CV to evaluate model
scores = cross_val_score(lr, x_lr, y, scoring='r2',
                         cv=cv, n_jobs=-1)
r2 = np.mean(scores)
print('the r2 of linear regression is: ', r2)

"""lasso """

cv = KFold(n_splits=10, random_state=1, shuffle=True)
#build multiple linear regression model
lasso_mse=[]
for i in np.arange(0,0.2,0.0005):
  lasso = Lasso(alpha = i)
  #use k-fold CV to evaluate model
  scores = cross_val_score(lasso, x, y, scoring='neg_mean_squared_error',
                         cv=cv, n_jobs=-1)
  lasso_mse.append(np.mean(- scores))

plt.plot(list(np.arange(0,0.2,0.0005)), lasso_mse, color='b', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=6)
plt.xlabel('lambda')
plt.ylabel('score')

print("lambda: ", lasso_mse.index(min(lasso_mse))*0.0005)
print("mse: ", min(lasso_mse))

cv = KFold(n_splits=10, random_state=1, shuffle=True)
#build multiple linear regression model
lasso_mse=[]
for i in np.arange(0,0.2,0.0005):
  lasso = Lasso(alpha = i)
  #use k-fold CV to evaluate model
  scores = cross_val_score(lasso, x, y, scoring='r2',
                         cv=cv, n_jobs=-1)
  lasso_mse.append(np.mean(scores))

plt.plot(list(np.arange(0,0.2,0.0005)), lasso_mse, color='b', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=6)
plt.xlabel('lambda')
plt.ylabel('score')

max(lasso_mse)

"""regression tree"""

cver = KFold(n_splits=10, shuffle=True, random_state=0) # K-fold CV environment object
folds = tuple(cver.split(x, y)) # splits into training and testing samples

def GridSearch(estimator, estimator_params):
    return GridSearchCV(estimator, estimator_params, cv=folds,
                        scoring='neg_mean_squared_error', return_train_score=True)

tree = DecisionTreeRegressor(random_state=0)
tree_params = {'max_depth': range(1, 25)}  # dictionary of hyperparameters of the tree
tree_cv = GridSearch(tree, tree_params)
tree_cv.fit(x, y)
tree_cv_results = pd.DataFrame(tree_cv.cv_results_)  # estimation results transformed to a pd.DataFrame

plt.plot(tree_cv_results['param_max_depth'], -tree_cv_results['mean_test_score'], label='test error')
plt.plot(tree_cv_results['param_max_depth'], -tree_cv_results['mean_train_score'], label='training error')
plt.legend()
plt.show()

tree_cv_results

print(tree_cv.best_params_, tree_cv.best_score_)

"""max_depth: 17 ;mse: 93253.090449

random forest
"""

max_depth_values = range(24,40)
n_estimators_values = range(50,51)
rf = RandomForestRegressor(max_features='sqrt', random_state=0)
rf_params = {'n_estimators': n_estimators_values, 'max_depth': max_depth_values}
rf_cv = GridSearch(rf, rf_params)
rf_cv.fit(x, y)
'''
rf_cv_results = pd.DataFrame(rf_cv.cv_results_)

for depth in max_depth_values:
    results = rf_cv_results[rf_cv_results['param_max_depth'] == depth]
    plt.plot(results['param_n_estimators'], -results['mean_test_score'],
            label='max_depth=%s' % depth)
plt.legend()
plt.show()
'''

print(rf_cv.best_params_, rf_cv.best_score_)

max_depth_values = range(32,33)
n_estimators_values = range(1,101)
rf = RandomForestRegressor(max_features='sqrt', random_state=0)
rf_params = {'n_estimators': n_estimators_values, 'max_depth': max_depth_values}
rf_cv = GridSearch(rf, rf_params)
rf_cv.fit(x, y)
rf_cv_results = pd.DataFrame(rf_cv.cv_results_)

for depth in max_depth_values:
    results = rf_cv_results[rf_cv_results['param_max_depth'] == depth]
    plt.plot(results['param_n_estimators'], -results['mean_test_score'],
            label='max_depth=%s' % depth)
plt.legend()
plt.show()

print(rf_cv.best_params_, rf_cv.best_score_)

"""max_depth: 32, mse: 60952.22667642568

xgboost
"""

xgbe = XGBRegressor(random_state=0, verbosity = 0)
n_estimators_values = range(50, 51)
xgbe_params = {'n_estimators': n_estimators_values, 'max_depth': range(15,30)}  # dictionary of hyperparameters of the tree
xgbe_cv = GridSearch(xgbe, xgbe_params)
xgbe_cv.fit(x, y)

print(xgbe_cv.best_params_, xgbe_cv.best_score_)

xgbe = XGBRegressor(random_state=0, verbosity = 0)
n_estimators_values = range(50, 51)
xgbe_params_1 = {'n_estimators': n_estimators_values, 'max_depth': range(1, 15)}  # dictionary of hyperparameters of the tree
xgbe_cv_1 = GridSearch(xgbe, xgbe_params_1)
xgbe_cv_1.fit(x, y)

print(xgbe_cv_1.best_params_, xgbe_cv_1.best_score_)

xgbe = XGBRegressor(random_state=0, verbosity = 0)
n_estimators_values = range(1, 101)
xgbe_params = {'n_estimators': n_estimators_values, 'max_depth': [10]}  # dictionary of hyperparameters of the tree
xgbe_cv = GridSearch(xgbe, xgbe_params)
xgbe_cv.fit(x, y)

xgbe_cv_results = pd.DataFrame(xgbe_cv.cv_results_)  # estimation results transformed to a pd.DataFrame

results = xgbe_cv_results['mean_test_score']
plt.plot(n_estimators_values, -results,)
plt.legend()
plt.show()

print(xgbe_cv.best_params_, xgbe_cv.best_score_)

from xgboost import plot_importance
xgbe_best = XGBRegressor(random_state=0, verbosity = 0, n_estimators = 92, max_depth = 10)
xgbe_best.fit(x,y)

ax = plot_importance(xgbe_best, height = 1)
fig = ax.figure
fig.set_size_inches(12, 10)
plt.show()